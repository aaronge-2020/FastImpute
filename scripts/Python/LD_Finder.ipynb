{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LD Proxy API to find the relevant positions for each of the PRS313 SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "# API key\n",
    "api_key = \"ac16be4ad92d\"\n",
    "\n",
    "# Base URL for LDProxy API\n",
    "base_url = \"https://ldlink.nih.gov/LDlinkRest/ldproxy\"\n",
    "\n",
    "population = \"ALL\"\n",
    "\n",
    "window = 1000000\n",
    "\n",
    "r2_threshold = 0.01\n",
    "\n",
    "# Read the chromosome positions from the text file\n",
    "PRS313_LD = pd.read_excel(\"../../Data/PRS313_with_23andMe.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate over each position\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m PRS313_LD:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min_23andMe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m          \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Extract chromosome and position from the line\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each position\n",
    "for sample in PRS313_LD:\n",
    "    if sample[\"in_23andMe\"] == True:\n",
    "         continue\n",
    "\n",
    "    # Extract chromosome and position from the line\n",
    "    chrom = sample[\"Chromosome\"]\n",
    "    pos = sample[\"Positionb\"]\n",
    "    \n",
    "    # Construct the API request URL\n",
    "    url = f\"{base_url}?var={chrom}:{pos}&pop={population}&r2_d=r2&window={window}&genome_build=grch37&token={api_key}\"\n",
    "    \n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "            # Create a StringIO object from the data\n",
    "            data_io = io.StringIO(response.text)\n",
    "\n",
    "            # Read the data into a DataFrame using read_csv\n",
    "            df = pd.read_csv(data_io, sep='\\\\t')\n",
    "            \n",
    "            # # Filter variants with high LD scores (e.g., R2 >= 0.8)\n",
    "            # high_ld_variants = df[df[\"R2\"].astype(float) >= 0.8]\n",
    "            \n",
    "            # Generate a unique filename for the CSV file\n",
    "            output_file = os.path.join(output_folder, f\"{chrom}_{pos}.csv\")\n",
    "            \n",
    "            # Save the high LD variants to a CSV file\n",
    "            df.to_csv(output_file, index=False)\n",
    "            \n",
    "            print(f\"Saved high LD variants for {chrom}:{pos} to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {chrom}:{pos}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom, pos = positions[0].strip().split()\n",
    "\n",
    "# Construct the API request URL\n",
    "url = f\"{base_url}?var={chrom}:{pos}&pop={population}&r2_d=r2&window={window}&genome_build=grch37&token={api_key}\"\n",
    "\n",
    "# Send the API request\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/982c10113957_2gb06y92y35sx509h/T/ipykernel_16385/1453726256.py:12: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(data_io, sep='\\\\t')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RS_Number</th>\n",
       "      <th>Coord</th>\n",
       "      <th>Alleles</th>\n",
       "      <th>MAF</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Dprime</th>\n",
       "      <th>R2</th>\n",
       "      <th>Correlated_Alleles</th>\n",
       "      <th>FORGEdb</th>\n",
       "      <th>RegulomeDB</th>\n",
       "      <th>Function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rs612683</td>\n",
       "      <td>chr1:100880328</td>\n",
       "      <td>(A/T)</td>\n",
       "      <td>0.4004</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>A=A,T=T</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rs12568038</td>\n",
       "      <td>chr1:100879914</td>\n",
       "      <td>(C/T)</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>-414</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.6162</td>\n",
       "      <td>A=C,T=T</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rs12564838</td>\n",
       "      <td>chr1:100883188</td>\n",
       "      <td>(A/G)</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>2860</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>A=A,T=G</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rs7517704</td>\n",
       "      <td>chr1:100885419</td>\n",
       "      <td>(A/G)</td>\n",
       "      <td>0.3494</td>\n",
       "      <td>5091</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>A=A,T=G</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rs12760924</td>\n",
       "      <td>chr1:100898600</td>\n",
       "      <td>(A/T)</td>\n",
       "      <td>0.3546</td>\n",
       "      <td>18272</td>\n",
       "      <td>0.8357</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>A=A,T=T</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>rs111334083</td>\n",
       "      <td>chr1:100617033</td>\n",
       "      <td>(G/A)</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>-263295</td>\n",
       "      <td>0.8351</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>rs141366089</td>\n",
       "      <td>chr1:100950426</td>\n",
       "      <td>(-/T)</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>70098</td>\n",
       "      <td>0.4970</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>rs10493930</td>\n",
       "      <td>chr1:101079706</td>\n",
       "      <td>(T/C)</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>199378</td>\n",
       "      <td>0.4070</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>rs57411954</td>\n",
       "      <td>chr1:100712556</td>\n",
       "      <td>(AC/-)</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>-167772</td>\n",
       "      <td>0.7285</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>rs7518386</td>\n",
       "      <td>chr1:101055617</td>\n",
       "      <td>(T/C)</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>175289</td>\n",
       "      <td>0.7502</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1219 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RS_Number           Coord Alleles     MAF  Distance  Dprime      R2  \\\n",
       "0        rs612683  chr1:100880328   (A/T)  0.4004         0  1.0000  1.0000   \n",
       "1      rs12568038  chr1:100879914   (C/T)  0.3415      -414  0.8908  0.6162   \n",
       "2      rs12564838  chr1:100883188   (A/G)  0.3496      2860  0.8609  0.5968   \n",
       "3       rs7517704  chr1:100885419   (A/G)  0.3494      5091  0.8609  0.5962   \n",
       "4      rs12760924  chr1:100898600   (A/T)  0.3546     18272  0.8357  0.5748   \n",
       "...           ...             ...     ...     ...       ...     ...     ...   \n",
       "1214  rs111334083  chr1:100617033   (G/A)  0.0212   -263295  0.8351  0.0101   \n",
       "1215  rs141366089  chr1:100950426   (-/T)  0.0575     70098  0.4970  0.0101   \n",
       "1216   rs10493930  chr1:101079706   (T/C)  0.0833    199378  0.4070  0.0100   \n",
       "1217   rs57411954  chr1:100712556  (AC/-)  0.0276   -167772  0.7285  0.0100   \n",
       "1218    rs7518386  chr1:101055617   (T/C)  0.0260    175289  0.7502  0.0100   \n",
       "\n",
       "     Correlated_Alleles  FORGEdb RegulomeDB Function  \n",
       "0               A=A,T=T      9.0         2b      NaN  \n",
       "1               A=C,T=T      6.0          7      NaN  \n",
       "2               A=A,T=G      6.0          5      NaN  \n",
       "3               A=A,T=G      6.0          6      NaN  \n",
       "4               A=A,T=T      6.0          6      NaN  \n",
       "...                 ...      ...        ...      ...  \n",
       "1214                NaN      7.0          5      NaN  \n",
       "1215                NaN      NaN          .      NaN  \n",
       "1216                NaN      6.0          5      NaN  \n",
       "1217                NaN      NaN          .      NaN  \n",
       "1218                NaN      4.0         3a      NaN  \n",
       "\n",
       "[1219 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Remove the leading single quote\n",
    "data = response.text\n",
    "\n",
    "# Create a StringIO object from the data\n",
    "data_io = io.StringIO(data)\n",
    "\n",
    "# Read the data into a DataFrame using read_csv\n",
    "df = pd.read_csv(data_io, sep='\\\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find overlap b/w 23AndMe and LD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 12_83064195.csv to ../../Data/ld_variants\\chr12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Check if the file is a CSV file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Extract the chromosome number and position from the filename\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         chrom, pos \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m         pos \u001b[38;5;241m=\u001b[39m pos\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Create a folder for the chromosome number\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Separate files in one directory into multiple folders based on chromosome number. The files are named chromosomeNumber_chromosomePosition.csv\n",
    "\n",
    "directory = \"../../Data/ld_variants\"\n",
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../../Data/ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a CSV file\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Extract the chromosome number and position from the filename\n",
    "        chrom, pos = filename.split(\"_\")\n",
    "        pos = pos.split(\".\")[0]\n",
    "        \n",
    "        # Create a folder for the chromosome number\n",
    "        chrom_folder = os.path.join(output_folder, \"chr\" + chrom)\n",
    "        os.makedirs(chrom_folder, exist_ok=True)\n",
    "        \n",
    "        # Move the file to the chromosome folder\n",
    "        src = os.path.join(directory, filename)\n",
    "        dest = os.path.join(chrom_folder, filename)\n",
    "        os.rename(src, dest)\n",
    "        \n",
    "        print(f\"Moved {filename} to {chrom_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Final Training Data with LD Proxy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LD for 12_14413931.csv because it is a known position. Column added to training data\n",
      "chr12_115796577_A_G_PRS313_Unknown\n",
      "Processed 12_115796577.csv\n",
      "Found 26 matching columns\n",
      "Skipping LD for 12_115835836.csv because it is a known position. Column added to training data\n",
      "Skipping LD for 12_120832146.csv because it is a known position. Column added to training data\n",
      "chr12_103097887_C_T_PRS313_Unknown\n",
      "Processed 12_103097887.csv\n",
      "Found 59 matching columns\n",
      "Skipping LD for 12_96027759.csv because it is a known position. Column added to training data\n",
      "chr12_85004551_C_T_PRS313_Unknown\n",
      "Processed 12_85004551.csv\n",
      "Found 119 matching columns\n",
      "chr12_111600134_G_T_PRS313_Unknown\n",
      "Processed 12_111600134.csv\n",
      "Found 187 matching columns\n",
      "chr12_28149568_C_T_PRS313_Unknown\n",
      "Processed 12_28149568.csv\n",
      "Found 220 matching columns\n",
      "chr12_29140260_G_A_PRS313_Unknown\n",
      "Processed 12_29140260.csv\n",
      "Found 258 matching columns\n",
      "Skipping LD for 12_57146069.csv because it is a known position. Column added to training data\n",
      "chr12_293626_A_G_PRS313_Unknown\n",
      "Processed 12_293626.csv\n",
      "Found 278 matching columns\n",
      "chr12_70798355_A_T_PRS313_Unknown\n",
      "Processed 12_70798355.csv\n",
      "Found 336 matching columns\n",
      "Skipping LD for 12_28174817.csv because it is a known position. Column added to training data\n",
      "SNP 12_83064195 not found in dbSNP and cannot be proxied using LDProxy\n",
      "Added 203 columns to matching_columns for missing data position: 12_83064195\n",
      "Processed 12_83064195.csv\n",
      "Found 539 matching columns\n",
      "chr12_115108136_T_C_PRS313_Unknown\n",
      "Processed 12_115108136.csv\n",
      "Found 593 matching columns\n",
      "chr12_28347382_C_T_PRS313_Unknown\n",
      "Processed 12_28347382.csv\n",
      "Found 630 matching columns\n",
      "Saved to file ../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr12_matching.parquet\n",
      "Found 630 matching columns\n",
      "Found 423 matching variants\n",
      "chr15_66630569_G_A_PRS313_Unknown\n",
      "Processed 15_66630569.csv\n",
      "Found 54 matching columns\n",
      "chr15_100905819_A_C_PRS313_Unknown\n",
      "Processed 15_100905819.csv\n",
      "Found 103 matching columns\n",
      "Skipping LD for 15_67457698.csv because it is a known position. Column added to training data\n",
      "chr15_46680811_C_A_PRS313_Unknown\n",
      "Processed 15_46680811.csv\n",
      "Found 107 matching columns\n",
      "chr15_91512267_G_T_PRS313_Unknown\n",
      "Processed 15_91512267.csv\n",
      "Found 158 matching columns\n",
      "Skipping LD for 15_50694306.csv because it is a known position. Column added to training data\n",
      "chr15_75750383_T_C_PRS313_Unknown\n",
      "Processed 15_75750383.csv\n",
      "Found 197 matching columns\n",
      "Saved to file ../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr15_matching.parquet\n",
      "Found 197 matching columns\n",
      "Found 195 matching variants\n",
      "chr14_105213978_T_G_PRS313_Unknown\n",
      "Processed 14_105213978.csv\n",
      "Found 83 matching columns\n",
      "chr14_37228504_C_T_PRS313_Unknown\n",
      "Processed 14_37228504.csv\n",
      "Found 110 matching columns\n",
      "Skipping LD for 14_93070286.csv because it is a known position. Column added to training data\n",
      "chr14_37128564_C_A_PRS313_Unknown\n",
      "Processed 14_37128564.csv\n",
      "Found 144 matching columns\n",
      "Skipping LD for 14_91841069.csv because it is a known position. Column added to training data\n",
      "Skipping LD for 14_68660428.csv because it is a known position. Column added to training data\n",
      "Skipping LD for 14_68979835.csv because it is a known position. Column added to training data\n",
      "chr14_91751788_TC_T_PRS313_Unknown\n",
      "Processed 14_91751788.csv\n",
      "Found 164 matching columns\n",
      "Saved to file ../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr14_matching.parquet\n",
      "Found 164 matching columns\n",
      "Found 160 matching variants\n",
      "chr13_73806982_T_C_PRS313_Unknown\n",
      "Processed 13_73806982.csv\n",
      "Found 27 matching columns\n",
      "chr13_32839990_G_A_PRS313_Unknown\n",
      "Processed 13_32839990.csv\n",
      "Found 40 matching columns\n",
      "chr13_73960952_A_G_PRS313_Unknown\n",
      "Processed 13_73960952.csv\n",
      "Found 89 matching columns\n",
      "Skipping LD for 13_32972626.csv because it is a known position. Column added to training data\n",
      "chr13_43501356_A_G_PRS313_Unknown\n",
      "Processed 13_43501356.csv\n",
      "Found 110 matching columns\n",
      "Saved to file ../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr13_matching.parquet\n",
      "Found 110 matching columns\n",
      "Found 110 matching variants\n",
      "chr22_46283297_G_A_PRS313_Unknown\n",
      "Processed 22_46283297.csv\n",
      "Found 51 matching columns\n",
      "Skipping LD for 22_39343916.csv because it is a known position. Column added to training data\n",
      "SNP 22_38583315 not found in dbSNP and cannot be proxied using LDProxy\n",
      "Added 203 columns to matching_columns for missing data position: 22_38583315\n",
      "Processed 22_38583315.csv\n",
      "Found 255 matching columns\n",
      "Skipping LD for 22_40904707.csv because it is a known position. Column added to training data\n",
      "chr22_19766137_C_T_PRS313_Unknown\n",
      "Processed 22_19766137.csv\n",
      "Found 269 matching columns\n",
      "chr22_43433100_C_T_PRS313_Unknown\n",
      "Processed 22_43433100.csv\n",
      "Found 313 matching columns\n",
      "chr22_45319953_G_A_PRS313_Unknown\n",
      "Processed 22_45319953.csv\n",
      "Found 348 matching columns\n",
      "chr22_29203724_C_T_PRS313_Unknown\n",
      "chr22_29551872_A_G_PRS313_Unknown\n",
      "Processed 22_29203724.csv\n",
      "Found 359 matching columns\n",
      "chr22_29135543_G_A_PRS313_Unknown\n",
      "Processed 22_29135543.csv\n",
      "Found 388 matching columns\n",
      "Skipping LD for 22_29121087.csv because it is a known position. Column added to training data\n",
      "Processed 22_29551872.csv\n",
      "Found 397 matching columns\n",
      "Saved to file ../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr22_matching.parquet\n",
      "Found 397 matching columns\n",
      "Found 191 matching variants\n",
      "chr4_106069013_G_T_PRS313_Unknown\n",
      "Processed 4_106069013.csv\n",
      "Found 100 matching columns\n",
      "chr4_38784633_G_T_PRS313_Unknown\n",
      "Processed 4_38784633.csv\n",
      "Found 151 matching columns\n",
      "chr4_175847436_C_A_PRS313_Unknown\n",
      "chr4_175842495_G_A_PRS313_Unknown\n",
      "Processed 4_175842495.csv\n",
      "Found 199 matching columns\n",
      "chr4_151218296_CATATTT_C_PRS313_Unknown\n",
      "Processed 4_151218296.csv\n",
      "Found 248 matching columns\n",
      "chr4_89240476_G_A_PRS313_Unknown\n",
      "Processed 4_89240476.csv\n",
      "Found 339 matching columns\n",
      "chr4_143467195_C_T_PRS313_Unknown\n",
      "Processed 4_143467195.csv\n",
      "Found 362 matching columns\n",
      "SNP 4_187503758 not found in dbSNP and cannot be proxied using LDProxy\n",
      "Added 405 columns to matching_columns for missing data position: 4_187503758\n",
      "Processed 4_187503758.csv\n",
      "Found 767 matching columns\n",
      "Processed 4_175847436.csv\n",
      "Found 782 matching columns\n",
      "SNP 4_126752992 not found in dbSNP and cannot be proxied using LDProxy\n",
      "Added 211 columns to matching_columns for missing data position: 4_126752992\n",
      "Processed 4_126752992.csv\n",
      "Found 993 matching columns\n",
      "chr4_92594859_TTCTTTC_T_PRS313_Unknown\n",
      "Processed 4_92594859.csv\n",
      "Found 1033 matching columns\n",
      "Skipping LD for 4_84370124.csv because it is a known position. Column added to training data\n",
      "Saved to file ../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr4_matching.parquet\n",
      "Found 1034 matching columns\n",
      "Found 417 matching variants\n",
      "SNP 3_55970777 not found in dbSNP and cannot be proxied using LDProxy\n",
      "Added 203 columns to matching_columns for missing data position: 3_55970777\n",
      "Processed 3_55970777.csv\n",
      "Found 203 matching columns\n",
      "chr3_59373745_C_T_PRS313_Unknown\n",
      "Processed 3_59373745.csv\n",
      "Found 229 matching columns\n",
      "chr3_141112859_CTT_C_PRS313_Unknown\n",
      "Processed 3_141112859.csv\n",
      "Found 326 matching columns\n",
      "chr3_99403877_G_A_PRS313_Unknown\n",
      "Processed 3_99403877.csv\n",
      "Found 385 matching columns\n",
      "Skipping LD for 3_172285237.csv because it is a known position. Column added to training data\n",
      "chr3_27388664_C_G_PRS313_Unknown\n",
      "Processed 3_27388664.csv\n",
      "Found 446 matching columns\n",
      "chr3_4742251_A_G_PRS313_Unknown\n",
      "Processed 3_4742251.csv\n",
      "Found 480 matching columns\n",
      "Skipping LD for 3_27353716.csv because it is a known position. Column added to training data\n",
      "chr3_29294845_C_T_PRS313_Unknown\n",
      "Processed 3_29294845.csv\n",
      "Found 482 matching columns\n",
      "chr3_30684907_C_T_PRS313_Unknown\n",
      "Processed 3_30684907.csv\n",
      "Found 522 matching columns\n",
      "chr3_46888198_T_C_PRS313_Unknown\n",
      "Processed 3_46888198.csv\n",
      "Found 556 matching columns\n",
      "chr3_71620370_T_G_PRS313_Unknown\n",
      "Processed 3_71620370.csv\n",
      "Found 616 matching columns\n",
      "chr3_49709912_C_CT_PRS313_Unknown\n",
      "Processed 3_49709912.csv\n",
      "Found 660 matching columns\n",
      "chr3_189774456_C_T_PRS313_Unknown\n",
      "Processed 3_189774456.csv\n",
      "Found 691 matching columns\n",
      "SNP 3_63887449 not found in dbSNP and cannot be proxied using LDProxy\n",
      "Added 255 columns to matching_columns for missing data position: 3_63887449\n",
      "Processed 3_63887449.csv\n",
      "Found 946 matching columns\n",
      "Skipping LD for 3_87037543.csv because it is a known position. Column added to training data\n",
      "Saved to file ../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr3_matching.parquet\n",
      "Found 947 matching columns\n",
      "Found 487 matching variants\n",
      "chr2_218292158_C_G_PRS313_Unknown\n",
      "Processed 2_218292158.csv\n",
      "Found 34 matching columns\n",
      "Skipping LD for 2_217920769.csv because it is a known position. Column added to training data\n",
      "chr2_172974566_C_G_PRS313_Unknown\n",
      "Processed 2_172974566.csv\n",
      "Found 78 matching columns\n",
      "chr2_88358825_G_C_PRS313_Unknown\n",
      "Processed 2_88358825.csv\n",
      "Found 91 matching columns\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coord \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m alleles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;66;03m# Got rid of allele matching, because of coding differences. GC--> C (in PRS313) is coded as (C/-) in dbSNP and LDProxy\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m         mask \u001b[38;5;241m=\u001b[39m (\u001b[43mvariants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCoord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m)\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m# mask = (variants['Coord'] == coord) & (variants['Alleles'] == alleles)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;66;03m# Only append column if it has not been added before\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:5799\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5796\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   5797\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 5799\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:131\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "directory = \"../../Data/ld_variants\"\n",
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../../Data/ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_coord_alleles(col_name):\n",
    "    match = re.search(r'chr(\\d+)_(\\d+)_([ACGT]+)_([ACGT,]+)', col_name)\n",
    "    if match:\n",
    "        chr_num, position, ref_allele, alt_alleles = match.groups()\n",
    "        return f'chr{chr_num}:{position}', f'({ref_allele}/{alt_alleles})'\n",
    "    return None, None\n",
    "\n",
    "# Create an empty list to store the matching variants across all chromosomes\n",
    "matching_variants_all = []\n",
    "\n",
    "# Iterate over each chromosome folder\n",
    "for chrom_folder in os.listdir(output_folder):\n",
    "    # Check if the folder is a chromosome folder\n",
    "    if chrom_folder.startswith(\"chr\"):\n",
    "        # Extract the chromosome number\n",
    "        chrom = chrom_folder[3:]\n",
    "\n",
    "        files_folder = os.path.join(output_folder, chrom_folder)\n",
    "        \n",
    "        # Load the training data for the chromosome\n",
    "        training_data = pd.read_parquet(f\"../../Data/Raw_training_data/23AndMe_PRS313_merged_chr{chrom}.parquet\")\n",
    "        \n",
    "        # Create empty lists to store the matching columns and variants for the chromosome\n",
    "        matching_columns = []\n",
    "        matching_variants_chrom = []\n",
    "        not_found_snps_chrom = []\n",
    "\n",
    "        # Read all the files in the folder\n",
    "        for filename in os.listdir(files_folder):\n",
    "\n",
    "            # Check if the file is a CSV file\n",
    "            if filename.endswith(\".csv\"):\n",
    "\n",
    "                # Check if the position is an unknown position or a known position \n",
    "                position = filename.split('.')[0]\n",
    "\n",
    "                # Find the column name with the position\n",
    "                position_column = training_data.columns[training_data.columns.str.contains(position)]\n",
    "\n",
    "                if (position_column[0].split(\"_\")[-1] == \"Known\"):\n",
    "                    # If the position is a known position, skip the file, because we don't need LD proxies for known positions\n",
    "                    print(f\"Skipping LD for {filename} because it is a known position. Column added to training data\")\n",
    "\n",
    "                    if position_column[0] not in matching_columns:\n",
    "\n",
    "                        # Add the matching column to the list\n",
    "                        matching_columns.append(position_column[0])\n",
    "\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Load the variants for the file\n",
    "                variants = pd.read_csv(os.path.join(files_folder, filename))\n",
    "                \n",
    "                # Find the matching columns in the training data\n",
    "                for column in training_data.columns:\n",
    "                    coord, alleles = extract_coord_alleles(column)\n",
    "                    if coord is not None and alleles is not None:\n",
    "                        try:\n",
    "                            # Got rid of allele matching, because of coding differences. GC--> C (in PRS313) is coded as (C/-) in dbSNP and LDProxy\n",
    "                            mask = (variants['Coord'] == coord)\n",
    "                            # mask = (variants['Coord'] == coord) & (variants['Alleles'] == alleles)\n",
    "                                \n",
    "                            if mask.any():\n",
    "                                # Only append column if it has not been added before\n",
    "                                if column not in matching_columns:\n",
    "\n",
    "                                    # Check column added is an unknown position\n",
    "                                    if (column.split(\"_\")[-1] == \"Unknown\"):\n",
    "                                        print(column)\n",
    "\n",
    "\n",
    "                                    matching_columns.append(column)\n",
    "                                    matching_variants_chrom.append(variants[mask])\n",
    "                        except KeyError:\n",
    "                            error_position = filename.split('.')[0]\n",
    "                            not_found_snps_chrom.append(error_position)\n",
    "                            print(f\"SNP {error_position} not found in dbSNP and cannot be proxied using LDProxy\")\n",
    "\n",
    "                            # Find the columns in training_data.columns with positions within +/- 500K BP of the error_position\n",
    "                            counter_error_added = 0\n",
    "                            for col in training_data.columns:\n",
    "                                coord, _ = extract_coord_alleles(col)\n",
    "                                if coord is not None:\n",
    "                                    col_position = int(coord.split(':')[1])\n",
    "                                    error_bp = int(error_position.split('_')[1])\n",
    "                                    if abs(col_position - error_bp) <= 500000 and col not in matching_columns:\n",
    "                                        counter_error_added += 1\n",
    "                                        matching_columns.append(col)\n",
    "\n",
    "                            print(f\"Added {counter_error_added} columns to matching_columns for missing data position: {error_position}\")\n",
    "                            \n",
    "                            break\n",
    "\n",
    "                print(f\"Processed {filename}\")\n",
    "                print(f\"Found {len(matching_columns)} matching columns\")\n",
    "                \n",
    "\n",
    "        # Get the matching columns from the training data\n",
    "        matching_data = training_data[matching_columns]\n",
    "\n",
    "        save_path = f\"../../Data/Filtered_training_data/23AndMe_PRS313_merged_chr{chrom}_matching.parquet\"\n",
    "        # Save the matching data for the chromosome\n",
    "        matching_data.to_parquet(save_path)\n",
    "\n",
    "        # Concatenate the matching variants for the chromosome into a single dataframe\n",
    "        matching_variants_chrom_df = pd.concat(matching_variants_chrom, ignore_index=True)\n",
    "        \n",
    "        # Append the matching variants for the chromosome to the overall list\n",
    "        matching_variants_all.append(matching_variants_chrom_df)\n",
    "\n",
    "        print(f\"Saved to file {save_path}\")\n",
    "        print(f\"Found {len(matching_columns)} matching columns\")\n",
    "        print(f\"Found {len(matching_variants_chrom_df)} matching variants\")\n",
    "\n",
    "# Concatenate the matching variants from all chromosomes into a single dataframe\n",
    "matching_variants_all_df = pd.concat(matching_variants_all, ignore_index=True)\n",
    "\n",
    "# Save the dataframe with matching variants across all chromosomes\n",
    "matching_variants_all_df.to_csv(\"../../Data/Filtered_training_data/23AndMe_matching_variants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../positions/Missing_ld.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m r2_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Read the RSIDs from the text file\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../positions/Missing_ld.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     18\u001b[0m     rsids \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create a folder to store the CSV files\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../positions/Missing_ld.txt'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "# API key\n",
    "api_key = \"ac16be4ad92d\"\n",
    "\n",
    "# Base URL for LDProxy API\n",
    "base_url = \"https://ldlink.nih.gov/LDlinkRest/ldproxy\"\n",
    "\n",
    "population = \"ALL\"\n",
    "window = 1000000\n",
    "r2_threshold = 0.01\n",
    "\n",
    "# Read the RSIDs from the text file\n",
    "with open(\"../positions/Missing_ld.txt\", \"r\") as file:\n",
    "    rsids = file.read().strip().split(\"\\n\")\n",
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each RSID\n",
    "for rsid in rsids:\n",
    "    # Construct the API request URL\n",
    "    url = f\"{base_url}?var={rsid}&pop={population}&r2_d=r2&window={window}&genome_build=grch37&token={api_key}\"\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Create a StringIO object from the data\n",
    "        data_io = io.StringIO(response.text)\n",
    "\n",
    "        # Read the data into a DataFrame using read_csv\n",
    "        df = pd.read_csv(data_io, sep='\\\\t')\n",
    "\n",
    "        # Generate a unique filename for the CSV file\n",
    "        output_file = os.path.join(output_folder, f\"{rsid}.csv\")\n",
    "\n",
    "        # Save the high LD variants to a CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Saved high LD variants for {rsid} to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {rsid}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Final Training Data with +/- 500K BP Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4138\n",
      "3266\n",
      "3217\n",
      "2468\n",
      "5178\n",
      "3592\n",
      "2290\n",
      "2740\n",
      "1953\n",
      "2854\n",
      "3720\n",
      "2333\n",
      "861\n",
      "806\n",
      "1252\n",
      "3042\n",
      "1565\n",
      "1537\n",
      "1847\n",
      "1127\n",
      "575\n",
      "2007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "window_size = 250000\n",
    "pattern = re.compile(r\"chr\\d+_(\\d+)_\")\n",
    "\n",
    "# Create an empty list to store the matching variants across all chromosomes\n",
    "matching_variants_all = []\n",
    "\n",
    "for chrom in range(1, 23):\n",
    "    # Load the training data for the chromosome\n",
    "    training_data = pd.read_parquet(f\"../../Data/Raw_training_data/23AndMe_PRS313_merged_chr{chrom}.parquet\")\n",
    "    \n",
    "    # Get all columns with \"PRS313\" in the name\n",
    "    prs313_unknown_columns = [col for col in training_data.columns if \"PRS313_Unknown\" in col]\n",
    "    prs313_unknown_positions = [int(pattern.search(col).group(1)) for col in prs313_unknown_columns]\n",
    "    prs313_unknown_positions_set = set(prs313_unknown_positions)\n",
    "    \n",
    "    # Get all columns in training_data that contain a number within +/- 500k of the PRS313_Unknown position\n",
    "    filtered_columns = [col for col in training_data.columns if any(abs(int(pattern.search(col).group(1)) - pos) <= window_size for pos in prs313_unknown_positions_set)]\n",
    "    \n",
    "    training_data_filtered = training_data[filtered_columns]\n",
    "\n",
    "    print(len(filtered_columns))\n",
    "\n",
    "    # Save the filtered training data for the chromosome\n",
    "    # training_data_filtered.to_parquet(f\"../../Data/500k_window_filtered_data/23AndMe_PRS313_merged_chr{chrom}_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2504, 2290)\n",
      "(2504, 444)\n"
     ]
    }
   ],
   "source": [
    "chrom = 7\n",
    "\n",
    "training_data_window = pd.read_parquet(f\"../../Data/500k_window_filtered_data/23AndMe_PRS313_merged_chr{chrom}_filtered.parquet\")\n",
    "print(training_data_window.shape)\n",
    "\n",
    "training_data_ld_proxy = pd.read_parquet(f\"../../Data/filtered_training_data/23AndMe_PRS313_merged_chr{chrom}_matching.parquet\")\n",
    "print(training_data_ld_proxy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chr1_88177403_G_A', 'chr1_88127152_T_C', 'chr1_88208135_G_A',\n",
       "       'chr1_88109828_G_A', 'chr1_88086894_C_A', 'chr1_88091734_C_T',\n",
       "       'chr1_88116467_A_C', 'chr1_88220810_AC_A', 'chr1_88073752_G_T',\n",
       "       'chr1_88120134_T_C',\n",
       "       ...\n",
       "       'chr1_172366806_A_G', 'chr1_172419651_T_G', 'chr1_172316842_G_A',\n",
       "       'chr1_171934790_G_A', 'chr1_172632057_A_G', 'chr1_172627498_C_T',\n",
       "       'chr1_172464519_T_G', 'chr1_172328767_T_TA_PRS313_Unknown',\n",
       "       'chr1_121280485_A_G', 'chr1_121137155_A_G'],\n",
       "      dtype='object', length=1132)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'chr1_88177403_G_A', 'chr1_88127152_T_C', 'chr1_88208135_G_A',\n",
    "       'chr1_88109828_G_A', 'chr1_88086894_C_A', 'chr1_88091734_C_T',\n",
    "       'chr1_88116467_A_C', 'chr1_88220810_AC_A', 'chr1_88073752_G_T',\n",
    "       'chr1_88120134_T_C',\n",
    "       ...\n",
    "       'chr1_172366806_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
