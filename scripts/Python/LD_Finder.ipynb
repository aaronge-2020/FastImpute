{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LD Proxy API to find the relevant positions for each of the PRS313 SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "# API key\n",
    "api_key = \"ac16be4ad92d\"\n",
    "\n",
    "# Base URL for LDProxy API\n",
    "base_url = \"https://ldlink.nih.gov/LDlinkRest/ldproxy\"\n",
    "\n",
    "population = \"ALL\"\n",
    "\n",
    "window = 1000000\n",
    "\n",
    "r2_threshold = 0.01\n",
    "\n",
    "# Read the chromosome positions from the text file\n",
    "PRS313_LD = pd.read_excel(\"../../Data/PRS313_with_23andMe.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate over each position\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m PRS313_LD:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min_23andMe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m          \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Extract chromosome and position from the line\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each position\n",
    "for sample in PRS313_LD:\n",
    "    if sample[\"in_23andMe\"] == True:\n",
    "         continue\n",
    "\n",
    "    # Extract chromosome and position from the line\n",
    "    chrom = sample[\"Chromosome\"]\n",
    "    pos = sample[\"Positionb\"]\n",
    "    \n",
    "    # Construct the API request URL\n",
    "    url = f\"{base_url}?var={chrom}:{pos}&pop={population}&r2_d=r2&window={window}&genome_build=grch37&token={api_key}\"\n",
    "    \n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "            # Create a StringIO object from the data\n",
    "            data_io = io.StringIO(response.text)\n",
    "\n",
    "            # Read the data into a DataFrame using read_csv\n",
    "            df = pd.read_csv(data_io, sep='\\\\t')\n",
    "            \n",
    "            # # Filter variants with high LD scores (e.g., R2 >= 0.8)\n",
    "            # high_ld_variants = df[df[\"R2\"].astype(float) >= 0.8]\n",
    "            \n",
    "            # Generate a unique filename for the CSV file\n",
    "            output_file = os.path.join(output_folder, f\"{chrom}_{pos}.csv\")\n",
    "            \n",
    "            # Save the high LD variants to a CSV file\n",
    "            df.to_csv(output_file, index=False)\n",
    "            \n",
    "            print(f\"Saved high LD variants for {chrom}:{pos} to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {chrom}:{pos}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom, pos = positions[0].strip().split()\n",
    "\n",
    "# Construct the API request URL\n",
    "url = f\"{base_url}?var={chrom}:{pos}&pop={population}&r2_d=r2&window={window}&genome_build=grch37&token={api_key}\"\n",
    "\n",
    "# Send the API request\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/982c10113957_2gb06y92y35sx509h/T/ipykernel_16385/1453726256.py:12: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(data_io, sep='\\\\t')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RS_Number</th>\n",
       "      <th>Coord</th>\n",
       "      <th>Alleles</th>\n",
       "      <th>MAF</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Dprime</th>\n",
       "      <th>R2</th>\n",
       "      <th>Correlated_Alleles</th>\n",
       "      <th>FORGEdb</th>\n",
       "      <th>RegulomeDB</th>\n",
       "      <th>Function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rs612683</td>\n",
       "      <td>chr1:100880328</td>\n",
       "      <td>(A/T)</td>\n",
       "      <td>0.4004</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>A=A,T=T</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rs12568038</td>\n",
       "      <td>chr1:100879914</td>\n",
       "      <td>(C/T)</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>-414</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.6162</td>\n",
       "      <td>A=C,T=T</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rs12564838</td>\n",
       "      <td>chr1:100883188</td>\n",
       "      <td>(A/G)</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>2860</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>A=A,T=G</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rs7517704</td>\n",
       "      <td>chr1:100885419</td>\n",
       "      <td>(A/G)</td>\n",
       "      <td>0.3494</td>\n",
       "      <td>5091</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>A=A,T=G</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rs12760924</td>\n",
       "      <td>chr1:100898600</td>\n",
       "      <td>(A/T)</td>\n",
       "      <td>0.3546</td>\n",
       "      <td>18272</td>\n",
       "      <td>0.8357</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>A=A,T=T</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>rs111334083</td>\n",
       "      <td>chr1:100617033</td>\n",
       "      <td>(G/A)</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>-263295</td>\n",
       "      <td>0.8351</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>rs141366089</td>\n",
       "      <td>chr1:100950426</td>\n",
       "      <td>(-/T)</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>70098</td>\n",
       "      <td>0.4970</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>rs10493930</td>\n",
       "      <td>chr1:101079706</td>\n",
       "      <td>(T/C)</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>199378</td>\n",
       "      <td>0.4070</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>rs57411954</td>\n",
       "      <td>chr1:100712556</td>\n",
       "      <td>(AC/-)</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>-167772</td>\n",
       "      <td>0.7285</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>rs7518386</td>\n",
       "      <td>chr1:101055617</td>\n",
       "      <td>(T/C)</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>175289</td>\n",
       "      <td>0.7502</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1219 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RS_Number           Coord Alleles     MAF  Distance  Dprime      R2  \\\n",
       "0        rs612683  chr1:100880328   (A/T)  0.4004         0  1.0000  1.0000   \n",
       "1      rs12568038  chr1:100879914   (C/T)  0.3415      -414  0.8908  0.6162   \n",
       "2      rs12564838  chr1:100883188   (A/G)  0.3496      2860  0.8609  0.5968   \n",
       "3       rs7517704  chr1:100885419   (A/G)  0.3494      5091  0.8609  0.5962   \n",
       "4      rs12760924  chr1:100898600   (A/T)  0.3546     18272  0.8357  0.5748   \n",
       "...           ...             ...     ...     ...       ...     ...     ...   \n",
       "1214  rs111334083  chr1:100617033   (G/A)  0.0212   -263295  0.8351  0.0101   \n",
       "1215  rs141366089  chr1:100950426   (-/T)  0.0575     70098  0.4970  0.0101   \n",
       "1216   rs10493930  chr1:101079706   (T/C)  0.0833    199378  0.4070  0.0100   \n",
       "1217   rs57411954  chr1:100712556  (AC/-)  0.0276   -167772  0.7285  0.0100   \n",
       "1218    rs7518386  chr1:101055617   (T/C)  0.0260    175289  0.7502  0.0100   \n",
       "\n",
       "     Correlated_Alleles  FORGEdb RegulomeDB Function  \n",
       "0               A=A,T=T      9.0         2b      NaN  \n",
       "1               A=C,T=T      6.0          7      NaN  \n",
       "2               A=A,T=G      6.0          5      NaN  \n",
       "3               A=A,T=G      6.0          6      NaN  \n",
       "4               A=A,T=T      6.0          6      NaN  \n",
       "...                 ...      ...        ...      ...  \n",
       "1214                NaN      7.0          5      NaN  \n",
       "1215                NaN      NaN          .      NaN  \n",
       "1216                NaN      6.0          5      NaN  \n",
       "1217                NaN      NaN          .      NaN  \n",
       "1218                NaN      4.0         3a      NaN  \n",
       "\n",
       "[1219 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Remove the leading single quote\n",
    "data = response.text\n",
    "\n",
    "# Create a StringIO object from the data\n",
    "data_io = io.StringIO(data)\n",
    "\n",
    "# Read the data into a DataFrame using read_csv\n",
    "df = pd.read_csv(data_io, sep='\\\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find overlap b/w 23AndMe and LD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 12_83064195.csv to ../../Data/ld_variants\\chr12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Check if the file is a CSV file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Extract the chromosome number and position from the filename\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         chrom, pos \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m         pos \u001b[38;5;241m=\u001b[39m pos\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Create a folder for the chromosome number\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Separate files in one directory into multiple folders based on chromosome number. The files are named chromosomeNumber_chromosomePosition.csv\n",
    "\n",
    "directory = \"../../Data/ld_variants\"\n",
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../../Data/ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a CSV file\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Extract the chromosome number and position from the filename\n",
    "        chrom, pos = filename.split(\"_\")\n",
    "        pos = pos.split(\".\")[0]\n",
    "        \n",
    "        # Create a folder for the chromosome number\n",
    "        chrom_folder = os.path.join(output_folder, \"chr\" + chrom)\n",
    "        os.makedirs(chrom_folder, exist_ok=True)\n",
    "        \n",
    "        # Move the file to the chromosome folder\n",
    "        src = os.path.join(directory, filename)\n",
    "        dest = os.path.join(chrom_folder, filename)\n",
    "        os.rename(src, dest)\n",
    "        \n",
    "        print(f\"Moved {filename} to {chrom_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Final Training Data with LD Proxy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LD for 12_14413931.csv because it is a known position. Column added to training data\n",
      "chr12_115796577_A_G_PRS313_Unknown\n",
      "Processed 12_115796577.csv\n",
      "Found 27 matching columns\n",
      "Skipping LD for 12_115835836.csv because it is a known position. Column added to training data\n",
      "Skipping LD for 12_120832146.csv because it is a known position. Column added to training data\n",
      "chr12_103097887_C_T_PRS313_Unknown\n",
      "Processed 12_103097887.csv\n",
      "Found 60 matching columns\n",
      "Skipping LD for 12_96027759.csv because it is a known position. Column added to training data\n",
      "chr12_85004551_C_T_PRS313_Unknown\n",
      "Processed 12_85004551.csv\n",
      "Found 121 matching columns\n",
      "chr12_111600134_G_T_PRS313_Unknown\n",
      "Processed 12_111600134.csv\n",
      "Found 191 matching columns\n",
      "chr12_28149568_C_T_PRS313_Unknown\n",
      "Processed 12_28149568.csv\n",
      "Found 224 matching columns\n",
      "chr12_29140260_G_A_PRS313_Unknown\n",
      "Processed 12_29140260.csv\n",
      "Found 264 matching columns\n",
      "Skipping LD for 12_57146069.csv because it is a known position. Column added to training data\n",
      "chr12_293626_A_G_PRS313_Unknown\n",
      "Processed 12_293626.csv\n",
      "Found 286 matching columns\n",
      "chr12_70798355_A_T_PRS313_Unknown\n",
      "Processed 12_70798355.csv\n",
      "Found 344 matching columns\n",
      "Skipping LD for 12_28174817.csv because it is a known position. Column added to training data\n",
      "SNP 12_83064195 not found in dbSNP and cannot be proxied using LDProxy\n",
      "Added 205 columns to matching_columns for missing data position: 12_83064195\n",
      "Processed 12_83064195.csv\n",
      "Found 549 matching columns\n",
      "chr12_115108136_T_C_PRS313_Unknown\n",
      "Processed 12_115108136.csv\n",
      "Found 603 matching columns\n",
      "chr12_28347382_C_T_PRS313_Unknown\n",
      "Processed 12_28347382.csv\n",
      "Found 641 matching columns\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '../../Data/Filtered_training_data_union'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Data/Filtered_training_data_union/23AndMe_PRS313_merged_chr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchrom\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_matching.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Save the matching data for the chromosome\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m \u001b[43mmatching_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Concatenate the matching variants for the chromosome into a single dataframe\u001b[39;00m\n\u001b[1;32m    118\u001b[0m matching_variants_chrom_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(matching_variants_chrom, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:2973\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2969\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parquet.py:483\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    481\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 483\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parquet.py:197\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     merged_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexisting_metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdf_metadata}\n\u001b[1;32m    195\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mreplace_schema_metadata(merged_metadata)\n\u001b[0;32m--> 197\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io\u001b[38;5;241m.\u001b[39mBufferedWriter)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[1;32m    208\u001b[0m ):\n\u001b[1;32m    209\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m path_or_handle\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 739\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '../../Data/Filtered_training_data_union'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "directory = \"../../Data/ld_variants\"\n",
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../../Data/ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_coord_alleles(col_name):\n",
    "    match = re.search(r'chr(\\d+)_(\\d+)_([ACGT]+)_([ACGT,]+)', col_name)\n",
    "    if match:\n",
    "        chr_num, position, ref_allele, alt_alleles = match.groups()\n",
    "        return f'chr{chr_num}:{position}', f'({ref_allele}/{alt_alleles})'\n",
    "    return None, None\n",
    "\n",
    "# Create an empty list to store the matching variants across all chromosomes\n",
    "matching_variants_all = []\n",
    "\n",
    "# Iterate over each chromosome folder\n",
    "for chrom_folder in os.listdir(output_folder):\n",
    "    # Check if the folder is a chromosome folder\n",
    "    if chrom_folder.startswith(\"chr\"):\n",
    "        # Extract the chromosome number\n",
    "        chrom = chrom_folder[3:]\n",
    "\n",
    "        files_folder = os.path.join(output_folder, chrom_folder)\n",
    "        \n",
    "        # Load the training data for the chromosome\n",
    "        training_data = pd.read_parquet(f\"../../Data/Raw_training_data_23andMe_union/23AndMe_PRS313_merged_chr{chrom}.parquet\")\n",
    "        \n",
    "        # Create empty lists to store the matching columns and variants for the chromosome\n",
    "        matching_columns = []\n",
    "        matching_variants_chrom = []\n",
    "        not_found_snps_chrom = []\n",
    "\n",
    "        # Read all the files in the folder\n",
    "        for filename in os.listdir(files_folder):\n",
    "\n",
    "            # Check if the file is a CSV file\n",
    "            if filename.endswith(\".csv\"):\n",
    "\n",
    "                # Check if the position is an unknown position or a known position \n",
    "                position = filename.split('.')[0]\n",
    "\n",
    "                # Find the column name with the position\n",
    "                position_column = training_data.columns[training_data.columns.str.contains(position)]\n",
    "\n",
    "                if (position_column[0].split(\"_\")[-1] == \"Known\"):\n",
    "                    # If the position is a known position, skip the file, because we don't need LD proxies for known positions\n",
    "                    print(f\"Skipping LD for {filename} because it is a known position. Column added to training data\")\n",
    "\n",
    "                    if position_column[0] not in matching_columns:\n",
    "\n",
    "                        # Add the matching column to the list\n",
    "                        matching_columns.append(position_column[0])\n",
    "\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Load the variants for the file\n",
    "                variants = pd.read_csv(os.path.join(files_folder, filename))\n",
    "                \n",
    "                # Find the matching columns in the training data\n",
    "                for column in training_data.columns:\n",
    "                    coord, alleles = extract_coord_alleles(column)\n",
    "                    if coord is not None and alleles is not None:\n",
    "                        try:\n",
    "                            # Got rid of allele matching, because of coding differences. GC--> C (in PRS313) is coded as (C/-) in dbSNP and LDProxy\n",
    "                            mask = (variants['Coord'] == coord)\n",
    "                            # mask = (variants['Coord'] == coord) & (variants['Alleles'] == alleles)\n",
    "                                \n",
    "                            if mask.any():\n",
    "                                # Only append column if it has not been added before\n",
    "                                if column not in matching_columns:\n",
    "\n",
    "                                    # Check column added is an unknown position\n",
    "                                    if (column.split(\"_\")[-1] == \"Unknown\"):\n",
    "                                        print(column)\n",
    "\n",
    "\n",
    "                                    matching_columns.append(column)\n",
    "                                    matching_variants_chrom.append(variants[mask])\n",
    "                        except KeyError:\n",
    "                            error_position = filename.split('.')[0]\n",
    "                            not_found_snps_chrom.append(error_position)\n",
    "                            print(f\"SNP {error_position} not found in dbSNP and cannot be proxied using LDProxy\")\n",
    "\n",
    "                            # Find the columns in training_data.columns with positions within +/- 500K BP of the error_position\n",
    "                            counter_error_added = 0\n",
    "                            for col in training_data.columns:\n",
    "                                coord, _ = extract_coord_alleles(col)\n",
    "                                if coord is not None:\n",
    "                                    col_position = int(coord.split(':')[1])\n",
    "                                    error_bp = int(error_position.split('_')[1])\n",
    "                                    if abs(col_position - error_bp) <= 500000 and col not in matching_columns:\n",
    "                                        counter_error_added += 1\n",
    "                                        matching_columns.append(col)\n",
    "\n",
    "                            print(f\"Added {counter_error_added} columns to matching_columns for missing data position: {error_position}\")\n",
    "                            \n",
    "                            break\n",
    "\n",
    "                print(f\"Processed {filename}\")\n",
    "                print(f\"Found {len(matching_columns)} matching columns\")\n",
    "                \n",
    "\n",
    "        # Get the matching columns from the training data\n",
    "        matching_data = training_data[matching_columns]\n",
    "\n",
    "        folder = \"../../Data/Filtered_training_data_union/\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        save_path = f\"{folder}/23AndMe_PRS313_merged_chr{chrom}_matching.parquet\"\n",
    "        # Save the matching data for the chromosome\n",
    "        matching_data.to_parquet(save_path)\n",
    "\n",
    "        # Concatenate the matching variants for the chromosome into a single dataframe\n",
    "        matching_variants_chrom_df = pd.concat(matching_variants_chrom, ignore_index=True)\n",
    "        \n",
    "        # Append the matching variants for the chromosome to the overall list\n",
    "        matching_variants_all.append(matching_variants_chrom_df)\n",
    "\n",
    "        print(f\"Saved to file {save_path}\")\n",
    "        print(f\"Found {len(matching_columns)} matching columns\")\n",
    "        print(f\"Found {len(matching_variants_chrom_df)} matching variants\")\n",
    "\n",
    "# Concatenate the matching variants from all chromosomes into a single dataframe\n",
    "matching_variants_all_df = pd.concat(matching_variants_all, ignore_index=True)\n",
    "\n",
    "# Save the dataframe with matching variants across all chromosomes\n",
    "matching_variants_all_df.to_csv(\"../../Data/Filtered_training_data/23AndMe_matching_variants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../positions/Missing_ld.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m r2_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Read the RSIDs from the text file\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../positions/Missing_ld.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     18\u001b[0m     rsids \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create a folder to store the CSV files\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../positions/Missing_ld.txt'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "# API key\n",
    "api_key = \"ac16be4ad92d\"\n",
    "\n",
    "# Base URL for LDProxy API\n",
    "base_url = \"https://ldlink.nih.gov/LDlinkRest/ldproxy\"\n",
    "\n",
    "population = \"ALL\"\n",
    "window = 1000000\n",
    "r2_threshold = 0.01\n",
    "\n",
    "# Read the RSIDs from the text file\n",
    "with open(\"../positions/Missing_ld.txt\", \"r\") as file:\n",
    "    rsids = file.read().strip().split(\"\\n\")\n",
    "\n",
    "# Create a folder to store the CSV files\n",
    "output_folder = \"../ld_variants\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each RSID\n",
    "for rsid in rsids:\n",
    "    # Construct the API request URL\n",
    "    url = f\"{base_url}?var={rsid}&pop={population}&r2_d=r2&window={window}&genome_build=grch37&token={api_key}\"\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Create a StringIO object from the data\n",
    "        data_io = io.StringIO(response.text)\n",
    "\n",
    "        # Read the data into a DataFrame using read_csv\n",
    "        df = pd.read_csv(data_io, sep='\\\\t')\n",
    "\n",
    "        # Generate a unique filename for the CSV file\n",
    "        output_file = os.path.join(output_folder, f\"{rsid}.csv\")\n",
    "\n",
    "        # Save the high LD variants to a CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Saved high LD variants for {rsid} to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {rsid}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Final Training Data with +/- 500K BP Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4138\n",
      "3266\n",
      "3217\n",
      "2468\n",
      "5178\n",
      "3592\n",
      "2290\n",
      "2740\n",
      "1953\n",
      "2854\n",
      "3720\n",
      "2333\n",
      "861\n",
      "806\n",
      "1252\n",
      "3042\n",
      "1565\n",
      "1537\n",
      "1847\n",
      "1127\n",
      "575\n",
      "2007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "window_size = 250000\n",
    "pattern = re.compile(r\"chr\\d+_(\\d+)_\")\n",
    "\n",
    "# Create an empty list to store the matching variants across all chromosomes\n",
    "matching_variants_all = []\n",
    "\n",
    "for chrom in range(1, 23):\n",
    "    # Load the training data for the chromosome\n",
    "    training_data = pd.read_parquet(f\"../../Data/Raw_training_data/23AndMe_PRS313_merged_chr{chrom}.parquet\")\n",
    "    \n",
    "    # Get all columns with \"PRS313\" in the name\n",
    "    prs313_unknown_columns = [col for col in training_data.columns if \"PRS313_Unknown\" in col]\n",
    "    prs313_unknown_positions = [int(pattern.search(col).group(1)) for col in prs313_unknown_columns]\n",
    "    prs313_unknown_positions_set = set(prs313_unknown_positions)\n",
    "    \n",
    "    # Get all columns in training_data that contain a number within +/- 500k of the PRS313_Unknown position\n",
    "    filtered_columns = [col for col in training_data.columns if any(abs(int(pattern.search(col).group(1)) - pos) <= window_size for pos in prs313_unknown_positions_set)]\n",
    "    \n",
    "    training_data_filtered = training_data[filtered_columns]\n",
    "\n",
    "    print(len(filtered_columns))\n",
    "\n",
    "    # Save the filtered training data for the chromosome\n",
    "    # training_data_filtered.to_parquet(f\"../../Data/500k_window_filtered_data/23AndMe_PRS313_merged_chr{chrom}_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2504, 2290)\n",
      "(2504, 444)\n"
     ]
    }
   ],
   "source": [
    "chrom = 7\n",
    "\n",
    "training_data_window = pd.read_parquet(f\"../../Data/500k_window_filtered_data/23AndMe_PRS313_merged_chr{chrom}_filtered.parquet\")\n",
    "print(training_data_window.shape)\n",
    "\n",
    "training_data_ld_proxy = pd.read_parquet(f\"../../Data/filtered_training_data/23AndMe_PRS313_merged_chr{chrom}_matching.parquet\")\n",
    "print(training_data_ld_proxy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chr1_88177403_G_A', 'chr1_88127152_T_C', 'chr1_88208135_G_A',\n",
       "       'chr1_88109828_G_A', 'chr1_88086894_C_A', 'chr1_88091734_C_T',\n",
       "       'chr1_88116467_A_C', 'chr1_88220810_AC_A', 'chr1_88073752_G_T',\n",
       "       'chr1_88120134_T_C',\n",
       "       ...\n",
       "       'chr1_172366806_A_G', 'chr1_172419651_T_G', 'chr1_172316842_G_A',\n",
       "       'chr1_171934790_G_A', 'chr1_172632057_A_G', 'chr1_172627498_C_T',\n",
       "       'chr1_172464519_T_G', 'chr1_172328767_T_TA_PRS313_Unknown',\n",
       "       'chr1_121280485_A_G', 'chr1_121137155_A_G'],\n",
       "      dtype='object', length=1132)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'chr1_88177403_G_A', 'chr1_88127152_T_C', 'chr1_88208135_G_A',\n",
    "       'chr1_88109828_G_A', 'chr1_88086894_C_A', 'chr1_88091734_C_T',\n",
    "       'chr1_88116467_A_C', 'chr1_88220810_AC_A', 'chr1_88073752_G_T',\n",
    "       'chr1_88120134_T_C',\n",
    "       ...\n",
    "       'chr1_172366806_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
